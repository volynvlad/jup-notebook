{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jupyter/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/jupyter/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/jupyter/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/jupyter/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/jupyter/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/jupyter/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "/home/jupyter/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/jupyter/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/jupyter/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/jupyter/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/jupyter/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/jupyter/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-2-9afd43a95c9d>:3: read_data_sets (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
      "WARNING:tensorflow:From /home/jupyter/.local/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:260: maybe_download (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please write your own downloading logic.\n",
      "WARNING:tensorflow:From /home/jupyter/.local/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:262: extract_images (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting /home/jupyter/.data/mnist/train-images-idx3-ubyte.gz\n",
      "WARNING:tensorflow:From /home/jupyter/.local/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:267: extract_labels (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting /home/jupyter/.data/mnist/train-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From /home/jupyter/.local/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:110: dense_to_one_hot (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.one_hot on tensors.\n",
      "Extracting /home/jupyter/.data/mnist/t10k-images-idx3-ubyte.gz\n",
      "Extracting /home/jupyter/.data/mnist/t10k-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From /home/jupyter/.local/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:290: DataSet.__init__ (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n"
     ]
    }
   ],
   "source": [
    "# import mnist data\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\"/home/jupyter/.data/mnist\", one_hot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define parameters\n",
    "element_size = 28\n",
    "time_steps = 28\n",
    "num_classes = 10\n",
    "batch_size = 128\n",
    "hidden_layer_size = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "LOG_DIR = \"./logs/RNN_with_summaries\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "_inputs = tf.placeholder(tf.float32, shape=\n",
    "                          [None, time_steps, element_size],\n",
    "                         name='inputs')\n",
    "y = tf.placeholder(tf.float32, shape=\n",
    "                  [None, num_classes], name='labels')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_x, batch_y = mnist.train.next_batch(batch_size)\n",
    "batch_x = batch_x.reshape((batch_size, time_steps, element_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this helper function, taken from the official TensorFlow docs,\n",
    "# simply adds some ops that take care of logging summaries\n",
    "def variable_summaries(var):\n",
    "    with tf.name_scope('summaries'):\n",
    "        mean = tf.reduce_mean(var)\n",
    "        tf.summary.scalar('mean', mean)\n",
    "        with tf.name_scope('stddev'):\n",
    "            stddev = tf.sqrt(tf.reduce_mean(tf.square(var - mean)))\n",
    "        tf.summary.scalar('stddev', stddev)\n",
    "        tf.summary.scalar('max', tf.reduce_max(var))\n",
    "        tf.summary.scalar('min', tf.reduce_min(var))\n",
    "        tf.summary.histogram('histogram', var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# weighs and bias for input and hidden layer\n",
    "with tf.name_scope('rnn_weights'):\n",
    "    with tf.name_scope('W_x'):\n",
    "        Wx = tf.Variable(tf.zeros([element_size, hidden_layer_size]))\n",
    "        variable_summaries(Wx)\n",
    "    with tf.name_scope('W_h'):\n",
    "        Wh = tf.Variable(tf.zeros([hidden_layer_size, hidden_layer_size]))\n",
    "        variable_summaries(Wh)\n",
    "    with tf.name_scope('Bias'):\n",
    "        b_rnn = tf.Variable(tf.zeros([hidden_layer_size]))\n",
    "        variable_summaries(b_rnn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rnn_step(previous_hidden_state, x):\n",
    "    current_hidden_state = tf.tanh(\n",
    "        tf.matmul(previous_hidden_state, Wh) +\n",
    "        tf.matmul(x, Wx) + b_rnn\n",
    "    )\n",
    "    return current_hidden_state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* processing inputs to work with scan function\n",
    "* current input shape: (batch_size, time_steps, element_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_input = tf.transpose(_inputs, perm=[1, 0, 2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* current input shape now: (time_steps, batch_size, element_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_hidden = tf.zeros([batch_size, hidden_layer_size])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* getting all state vectors across time\n",
    "* important to the first axis represents the time axis\n",
    "* we can iterate across all time steps by using the tf.scan()\n",
    "* it is applies a callable (function) to a sequence of elements in order\n",
    "* as explained in the folowing note"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_hidden_states = tf.scan(rnn_step,\n",
    "                           processed_input,\n",
    "                           initializer=initial_hidden,\n",
    "                           name='states')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### tf.scan()\n",
    "* This important function was added to TensorFlow to allow us to\n",
    "introduce loops into the computation graph, instead of just\n",
    "“unrolling” the loops explicitly by adding more and more replications of the same operations. More technically, it is a higherorder\n",
    "function very similar to the reduce operator, but it returns all intermediate accumulator values over time. There are several advantages to this approach, chief among them the ability to have a\n",
    "dynamic number of iterations rather than fixed, computational\n",
    "speedups and optimizations for graph construction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* In this case, we use tf.scan() to sequentially concatenate characters to a string, in a\n",
    "manner analogous to the arithmetic cumulative sum."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([b'T', b'Te', b'Ten', b'Tens', b'Tenso', b'Tensor', b'Tensor ',\n",
       "       b'Tensor F', b'Tensor Fl', b'Tensor Flo', b'Tensor Flow'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "elems = np.array(['T', 'e', 'n', 's', 'o', 'r', ' ', 'F', 'l', 'o', 'w'])\n",
    "scan_sum = tf.scan(lambda a, x: a + x, elems)\n",
    "\n",
    "sess = tf.InteractiveSession()\n",
    "sess.run(scan_sum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# weights for output layers\n",
    "with tf.name_scope('linear_layer_weights') as scope:\n",
    "    with tf.name_scope('W_linear'):\n",
    "        Wl = tf.Variable(tf.truncated_normal([hidden_layer_size,\n",
    "                                             num_classes],\n",
    "                                            mean=0, stddev=.01))\n",
    "        variable_summaries(Wl)\n",
    "    with tf.name_scope('Bias_linear'):\n",
    "        bl = tf.Variable(tf.truncated_normal([num_classes],\n",
    "                                            mean=0, stddev=.01))\n",
    "        variable_summaries(bl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply linear layer to state vector\n",
    "def get_linear_layer(hidden_state):\n",
    "    return tf.matmul(hidden_state, Wl) + bl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.name_scope('linear_layer_weights') as scope:\n",
    "    # iterate across time, apply linear layer to all RNN outputs\n",
    "    all_outputs = tf.map_fn(get_linear_layer, all_hidden_states)\n",
    "    # get last output\n",
    "    output = all_outputs[-1]\n",
    "    tf.summary.histogram('outputs', output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RNN classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-18-4a334928b542>:3: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See `tf.nn.softmax_cross_entropy_with_logits_v2`.\n",
      "\n",
      "WARNING:tensorflow:From /home/jupyter/.local/lib/python3.6/site-packages/tensorflow/python/training/rmsprop.py:119: calling Ones.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n"
     ]
    }
   ],
   "source": [
    "with tf.name_scope('cross_entropy'):\n",
    "    cross_entropy = tf.reduce_mean(\n",
    "        tf.nn.softmax_cross_entropy_with_logits(logits=output, labels=y)\n",
    "    )\n",
    "    tf.summary.scalar('cross_entropy', cross_entropy)\n",
    "\n",
    "with tf.name_scope('train'):\n",
    "    train_step = tf.train.RMSPropOptimizer(0.001, 0.9).minimize(cross_entropy)\n",
    "    \n",
    "with tf.name_scope('accuracy'):\n",
    "    correct_prediction = tf.equal(\n",
    "    tf.argmax(y, 1), tf.argmax(output, 1))\n",
    "    accuracy = (tf.reduce_mean(tf.cast(correct_prediction, tf.float32))) * 100\n",
    "    tf.summary.scalar('accuracy', accuracy)\n",
    "\n",
    "#merge all the summaries\n",
    "merged = tf.summary.merge_all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 0, Minibatch Loss= 2.302763, Training Accuracy= 12.50000\n",
      "Iter 1000, Minibatch Loss= 1.436974, Training Accuracy= 50.78125\n",
      "Iter 2000, Minibatch Loss= 0.632700, Training Accuracy= 80.46875\n",
      "Iter 3000, Minibatch Loss= 0.206060, Training Accuracy= 93.75000\n",
      "Iter 4000, Minibatch Loss= 0.264060, Training Accuracy= 93.75000\n",
      "Iter 5000, Minibatch Loss= 0.093984, Training Accuracy= 98.43750\n",
      "Iter 6000, Minibatch Loss= 0.077726, Training Accuracy= 99.21875\n",
      "Iter 7000, Minibatch Loss= 0.059325, Training Accuracy= 98.43750\n",
      "Iter 8000, Minibatch Loss= 0.022851, Training Accuracy= 99.21875\n",
      "Iter 9000, Minibatch Loss= 0.009136, Training Accuracy= 100.00000\n",
      "Test Accuracy: 97.65625, step: 9999\n"
     ]
    }
   ],
   "source": [
    "# get a small test set\n",
    "test_data = mnist.test.images[:batch_size].reshape((-1, time_steps,\n",
    "                                                   element_size))\n",
    "test_label = mnist.test.labels[:batch_size]\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    # Write summaries to LOG_DIR -- used by TensorBoard\n",
    "    train_writer = tf.summary.FileWriter(LOG_DIR + '/train',\n",
    "                                graph=tf.get_default_graph())\n",
    "    test_writer = tf.summary.FileWriter(LOG_DIR + '/test',\n",
    "                                graph=tf.get_default_graph())\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    for i in range(10000):\n",
    "        batch_x, batch_y = mnist.train.next_batch(batch_size)\n",
    "        # reshape data to get 28 sequences of 28 pixels\n",
    "        batch_x = batch_x.reshape((batch_size, time_steps, element_size))\n",
    "        summary,_ = sess.run([merged, train_step],\n",
    "                            feed_dict={_inputs:batch_x, y:batch_y})\n",
    "        # add to summaries\n",
    "        train_writer.add_summary(summary, i)\n",
    "\n",
    "        if i % 1000 == 0:\n",
    "            acc,loss, = sess.run([accuracy,cross_entropy],\n",
    "                                feed_dict={_inputs: batch_x,\n",
    "                                            y: batch_y})\n",
    "            print(\"Iter \" + str(i) + \", Minibatch Loss= \" + \\\n",
    "                \"{:.6f}\".format(loss) + \", Training Accuracy= \" + \\\n",
    "                \"{:.5f}\".format(acc))\n",
    "        if i % 10:\n",
    "            # Calculate accuracy for 128 MNIST test images and\n",
    "            # add to summaries\n",
    "            summary, acc = sess.run([merged, accuracy],\n",
    "                                   feed_dict={_inputs: test_data,\n",
    "                                             y: test_label})\n",
    "            test_writer.add_summary(summary, i)\n",
    "\n",
    "    test_acc = sess.run(accuracy, feed_dict={_inputs: test_data,\n",
    "                                                y: test_label})\n",
    "    print(\"Test Accuracy: {}, step: {}\".format(test_acc, i))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
